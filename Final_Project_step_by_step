Docker files and setup : 
    https://github.com/ajihsan/ds9-final-project 

Data Warehouse in dbeaver Postgres
    host : localhost
    port : 5432
    username : postgres
    password : 1234
    database : postgres
    schema : final_project

Data Warehouse in dbeaver MySQL
    host : localhost
    port : 3306
    username : root
    password : mysqlroot
    database : mysql
    schema : sys

Virtual environment python : final_project


Tasks:
1. Data Batch Processing : Using Spark

    ** Take data csv from kaggle
        https://www.kaggle.com/competitions/home-credit-default-risk/data?select=application_train.csv
        https://www.kaggle.com/competitions/home-credit-default-risk/data?select=application_test.csv
    ** Using Spark, read the csv, then upload to MySQL
        $ python3 spark/csv_to_mysql.py 
    ** Upload from MySQL to Postgres
        $ python3 spark/mysql_to_postgres.py 

2. Data Stream Processing : Using Kafka
    
    ** create topic TopicCurrency
        $ bin/kafka-topics.sh --create --topic TopicCurrency --bootstrap-server localhost:9092
    ** stream data from API: 
        https://freeforexapi.com/Home/Api        
        https://www.freeforexapi.com/api/live?pairs=EURUSD,EURGBP,USDEUR
    ** run for producer side 
        $ kafka_2.12-3.2.3/bin/kafka-console-producer.sh --topic TopicCurrency --bootstrap-server localhost:9092
    ** run for consumer side
        $ kafka_2.12-3.2.3/bin/kafka-console-consumer.sh --topic TopicCurrency --from-beginning --bootstrap-server localhost:9092
    ** run python script for producer
        $ python3 /home/agnes/Documents/digital_skola/Project/final_project/kafka/kafka_producer.py
    ** run python script for consumer
        $ python3 /home/agnes/Documents/digital_skola/Project/final_project/kafka/kafka_consumer.py

3. NoSQL Data Processing : Using MongoDB
    
    ** create connection with MongoDB using PyMongo
    ** for sample_training - zips : flatten loc to latitute and longitude --> table name: sample_training_zips
    ** for sample_training companies : take only non-array value + only first array of offices --> table name: sample_training_companies
    ** load table to postgres
    ** running command:
        $ python3 mongodb/mongodb_etl_zips.py 
        $ python3 mongodb/mongodb_etl_companies.py 
    
4. Create Dimension table
    ** using sql language:
    - Create dim country (id, country_code) 
    - Create dim state (id, country_id, state_code) 
    - Create dim city (id, state_id, city_name, zip_code)
    - Create dim currency (id, currency_name, currency_code)

5. Create Fact table 
    ** using sql language:
    - Create fact total city & office per state
    - Create fact currency daily avg (using macros previous day)
    - Create fact currency monthly avg (using macros previous month)

6. Machine Learning Train and Test data 
    ** get table from postgres
    ** do table cleaning and upload result to postgres
    ** do machine learning Linear Regression and upload result to postgres 

7. Put all work on airflow 
    ** setting postgres connection : airflow_final_project
    1. Create unscheduled DAG etl_mongodb for process of mongodb etl
    2. Create unscheduled DAG etl_spark for process of sql data etl using spark
    3. Create unscheduled DAG dim_table for process of creating all dimension table --> Dependency with DAG etl_mongodb and etl_spark
    4. Create daily DAG for daily fact tables --> Dependency DAG dim_table
    5. Create monthly DAG for monthly fact table --> Dependency DAG dim_table 


